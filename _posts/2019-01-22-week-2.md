---
title:  "Week 2 Writeup"
---

# Week 2

This week covered the basics of forensic computing, forensic tools, and techniques involved with forensic analysis.   
### Forensic computing defined
Forensic computing is the process of identifying, preserving, analyzing, and presenting digital evidence that is in accordance with the law. It's intent is to decipher what exactly happened with a machine, and presenting this to law enforcement for investigative purposes. In essence, one would gather evidence, investigate the evidence, and report it to some authoritative body. There are three general types of forensic computing, live forensics, post-mortem basedd forensics, and network based forensics.   
Live forensics is when one takes the compuer from a location and transports it somewhere else to do analysis. This is obsolete, as a substantial amount of evidence is lost when a computer is unplugged.   
Post-mortem forensics is preferred to live. This type of forensics seeks to gather evidence with memory in tact. This means that the investigator can either analyze on site, or take a memory dump and work with that dump elsewhere.   
Network forensics utilizes network logs to build a case. It is another effective way of doing forensics.   
### Evidence   
In the most simplest terms, evidence is anything that can be used to prove or disprove a fact. Regarding forensics, evidence can be found in many levels, such as the network, operating system, databases, accessories, removable media, or even human testimony. Evidence integrity must always be preserverd and counter measures should be taken to ensure it is not tampered with. For example, cryptographic hashes can be computed on evidence to ensure no changes are made. If a change is made to evidence, the hash will change, alerting the investigator to the change. Investigators can also try to ensure integrity by making evidence extremely hard to access, for example, by locking up hard drives in a vault.
### Volatility   
Volatility is a measure of how susceptible a piece of evidence is to being corrupted or lost. Usually the order of volatility is:   
  * System memory   
  * Temporary file systems   
  * Network connections and process tables   
  * Network routing and ARP cache   
  * Acquisitions of disks   
  * Remote logging and data monitoring   
  * physical configuration and network topography   
  * Backups
One should always take measures to ensure evidence is kept as free from errors as possible, because once evidence is contaminatied, it will stay contaminated. There is no way to decontaminate computational evidence. The closest thing would be to restore a backup. With this in mind, one should always prioritze collecting volatile data before nonvolatile data.   
### Tools   
Tools to gather evidence should always be used from removable media, such as a usb stick or cd. Some common tools are:   
  * **Volatility** - Used to create memory dumps. Collects all running processes, drivers, and modules at time of snapshot, registru keys, wireless keys, etc. Basically takes a picture of memory and all of it's included data.   
  * **MFTParser** - Parses Master File Table,  which contains information about every file and directory in a windoes environment.   
  * **Timeliner** - Used to generate system timelines on windows. Useful for seeing how malware changes a system through time.   
  * **OSFMount** - Mounts a filesystem so one can explore it. Useful for recovering files when used with Photorec.
  * **Photorec** - Tool used to recover various types of files. Useful when used in conjuction with OSFMount. Reads header and footer information to decipher what type of file would be stored at some point in the file system.   
  